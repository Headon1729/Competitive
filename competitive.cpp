#include<bits/stdc++.h>
using namespace std;

int main() {
#ifndef ONLINE_JUDGE
freopen("input.txt", "r", stdin);
freopen("output.txt", "w", stdout);
#endif
//--------------------------------------START----------------------------------------//
	int t; cin >> t;
	while(t--){
		

	}	
	return 0;
	
// ------------------------------------  END -------------------------------------//
}


/*
1.	Nested rendering --> this.state inner render change, does outer render also gets called 
2.	returning multiple sub-components -- accessdiv
3.  Exercise -- 



Machine Learning
1. Classification Problem : It's the process of building a model which could separate the data into multiple categories i.e. discrete values. Eg - Logistic regression, decision tree.

2.  Regression Problem : It's the process of building a model which could separate the . Eg - Linear regression

CLASSIFICATION 
In a classification problem, when will we say a feature is a good/dominant feature : When the difference of mean of vectors of two classes is large & difference of variance is minimum or take ratio of both.                                                                                                                                                                                               a. Linear Discriminant Analysis  is like PCA, but it focuses more on separability among the known categories.                                                                                1. Fischer Discriminator : Try to create a new axis on which after taking projection, the difference of mean is maximum & sum of variance is minimum.
b. Using Normal Distribution or Max. likelihood estimator : To classify a data-points into different sets which are not separable, we can use Normal distribution i.e. draw normal distribution of the two sets & take the break point the point where the two curves cut. To the left of the break-point would be the fist set & to the right would be the maximum likelihood of the other set. Take normal distribution if no distribution is provided. 
c. Perceptron Algorithm : If the points are linearly separable, then we can use hyperplane to achieve it & if it isn't LS, then it will go in a infinite loop.

Dimension Reduction : How to take the essence of 10 features to one feature.  









REGRESSION 
Gradient descent vs Stochastic gradient descent :  Here, graph needs to be convex

 Food for brain  : 

1. mathematics of logistic regression learnt

2. 













*/